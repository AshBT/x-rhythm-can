{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # only relevant to my own environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that you have downloaded MIDI dataset and uncompssed into `./midi_dataset/groove/` folder  \n",
    "https://magenta.tensorflow.org/datasets/groove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRUM_CLASSES = [\n",
    "   'Kick',\n",
    "   'Snare',\n",
    "   'Hi-hat closed',\n",
    "   'Hi-hat open',\n",
    "   'Tom low',\n",
    "   'Tom mid', \n",
    "   'Tom high',\n",
    "   'Clap',\n",
    "   'Rim' \n",
    "]\n",
    "MIDI_DRUM_MAP = {\n",
    "   36: 0,\n",
    "   35: 0,\n",
    "   38: 1,\n",
    "   27: 1,\n",
    "   28: 1,\n",
    "   31: 1,\n",
    "   32: 1,\n",
    "   33: 1,\n",
    "   34: 1,\n",
    "   37: 1,\n",
    "   39: 1,\n",
    "   40: 1,\n",
    "   56: 1,\n",
    "   65: 1,\n",
    "   66: 1,\n",
    "   75: 1,\n",
    "   85: 1,\n",
    "   42: 2,\n",
    "   44: 2,\n",
    "   54: 2,\n",
    "   68: 2,\n",
    "   69: 2,\n",
    "   70: 2,\n",
    "   71: 2,\n",
    "   73: 2,\n",
    "   78: 2,\n",
    "   80: 2,\n",
    "   46: 3,\n",
    "   67: 3,\n",
    "   72: 3,\n",
    "   74: 3,\n",
    "   79: 3,\n",
    "   81: 3,\n",
    "   45: 4,\n",
    "   29: 4,\n",
    "   41: 4,\n",
    "   61: 4,\n",
    "   64: 4,\n",
    "   84: 4,\n",
    "   48: 5,\n",
    "   47: 5,\n",
    "   60: 5,\n",
    "   63: 5,\n",
    "   77: 5,\n",
    "   86: 5,\n",
    "   87: 5,\n",
    "   50: 6,\n",
    "   30: 6,\n",
    "   43: 6,\n",
    "   62: 6,\n",
    "   76: 6,\n",
    "   83: 6,\n",
    "   49: 7,\n",
    "   55: 7,\n",
    "   57: 7,\n",
    "   58: 7,\n",
    "   51: 8,\n",
    "   52: 8,\n",
    "   53: 8,\n",
    "   59: 8,\n",
    "   82: 8\n",
    "}\n",
    "\n",
    "\n",
    "DRUM_MIDI_MAP = [ # pianoroll to MIDI - reverse\n",
    "    36, # 0 Kick\n",
    "    38, # 1 Snare\n",
    "    42, # 2 Hihat Closed\n",
    "    46, # 3 Hihat Open\n",
    "    45, # 4 Tom Low\n",
    "    47, # 5 Tom Mid\n",
    "    50, # 6 Tom High\n",
    "    49, # 7 Clap\n",
    "    51  # 8 Rim\n",
    "]\n",
    "\n",
    "resolution  = 4 # separate quater into 4  = 16 notes per bar\n",
    "\n",
    "nb_bars = 2 \n",
    "\n",
    "len_seq = resolution * 4 * nb_bars # length of drumloops in training data - 2 bars\n",
    "    \n",
    "nb_notes = len(DRUM_CLASSES) # number of possible MIDI notes  - max_drum_note - min_drum_note\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrices_drums = np.load(\"./tmp/matrices_drum_gm_electronic.npz\")['drum_data']\n",
    "matrices_genres = np.load(\"./tmp/matrices_drum_gm_electronic.npz\")['genre_ids']\n",
    "GENRES_ALL = np.load(\"./tmp/matrices_drum_gm_electronic.npz\")['genres']\n",
    "\n",
    "print(GENRES_ALL)\n",
    "print(matrices_genres.shape)\n",
    "\n",
    "# Filter genres \n",
    "GENRES_USED = [u'Old Skool', u'DnB', u'Jungle', u'House', u'Breakbeat', u'Garage', u'Techno']\n",
    "GENRES_ID_USED = [i for i, genre in enumerate(GENRES_ALL) if genre in GENRES_USED]\n",
    "GENRES = [genre for i, genre in enumerate(GENRES_ALL) if genre in GENRES_USED]\n",
    "\n",
    "print(GENRES_ID_USED)\n",
    "print(GENRES)\n",
    "\n",
    "NB_GENRES = len(GENRES)\n",
    "print(\"%d genres we have\" % NB_GENRES)\n",
    "\n",
    "_drums = []\n",
    "_genres = []\n",
    "\n",
    "for genre_id, drum in zip(matrices_genres, matrices_drums):\n",
    "    if genre_id in GENRES_ID_USED:\n",
    "        _drums.append(drum)\n",
    "        genre_id = GENRES_ID_USED.index(genre_id)\n",
    "        _genres.append(genre_id)\n",
    "\n",
    "matrices_drums = np.array(_drums)\n",
    "print(matrices_drums.shape)\n",
    "matrices_genres = np.array(_genres)\n",
    "print(matrices_genres.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, LSTM, Bidirectional, Lambda, Concatenate, Softmax\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "batch_size = 16\n",
    "n_z = 4\n",
    "dropout_rate = 0.30\n",
    "\n",
    "# encoder\n",
    "drum_input = Input(shape=(len_seq, nb_notes), name='drum_input')  # tensorflow order\n",
    "x = Bidirectional(LSTM(64, return_sequences=True, activation='tanh'))(drum_input) \n",
    "x = Bidirectional(LSTM(128, return_sequences=True, activation='tanh'))(x)\n",
    "x = Bidirectional(LSTM(16, return_sequences=True, activation='tanh'))(x)\n",
    "x1 = Reshape((1024,))(x)\n",
    "\n",
    "x = Dense(512)(x1)\n",
    "x = LeakyReLU(alpha=0.01)(x)\n",
    "x = Dense(256)(x)\n",
    "x = LeakyReLU(alpha=0.01)(x)\n",
    "output = Dense(1, activation='sigmoid', name='gan_output')(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x1)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(NB_GENRES)(x)\n",
    "genre_output = Softmax(name='style_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_c = Model(drum_input, [output, genre_output])\n",
    "discriminator_c.summary()\n",
    "\n",
    "optimizer = Adam() # higher leraning rate for D\n",
    "discriminator_c.compile(optimizer=optimizer, loss={'gan_output':'binary_crossentropy','style_output':'categorical_crossentropy'},\n",
    "                      loss_weights=[1.0, 0.5],\n",
    "                    metrics={'gan_output':'binary_accuracy', 'style_output':'categorical_accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary model \n",
    "discriminator_a = Model(drum_input, output)\n",
    "discriminator_a.summary()\n",
    "\n",
    "optimizer = Adam() # higher leraning rate for D\n",
    "discriminator_a.compile(optimizer=optimizer, \n",
    "                        loss={'gan_output':'binary_crossentropy'},\n",
    "                    metrics={'gan_output':'binary_accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model(\"./tmp/rhythm_classification_gm.h5\")\n",
    "classifier.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i, layer in enumerate(discriminator_c.layers):\n",
    "    print i, layer\n",
    "    \n",
    "    if i <= 4 or layer not in discriminator_a.layers:\n",
    "        w = classifier.layers[j].get_weights()\n",
    "        layer.set_weights(w)\n",
    "        layer.trainable = False # freezes the layer\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print discriminator_c.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Conv2DTranspose, RepeatVector, Activation,Bidirectional,Embedding,multiply\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "\n",
    "len_input = 100\n",
    "\n",
    "z_input = Input(shape=(len_input,))  # tensorflow order\n",
    "\n",
    "# Conditioning label\n",
    "label_input = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Embedding(NB_GENRES, len_input, input_length=1)(label_input)\n",
    "label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "# Element-wise product of the vectors z and the label embeddings\n",
    "inputs2 = multiply([z_input, label_embedding])\n",
    "\n",
    "# \n",
    "x = Dense(512)(inputs2)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = BatchNormalization(momentum=0.9)(x)\n",
    "x = Dense(1024)(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = BatchNormalization(momentum=0.9)(x)\n",
    "x = Reshape((32, 32))(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = LSTM(128, return_sequences=True, activation='tanh')(x) \n",
    "x = LSTM(128, return_sequences=True, activation='tanh')(x)\n",
    "note_out = LSTM(9, return_sequences=True, activation='relu')(x)\n",
    "#note_out = LeakyReLU()(x)\n",
    "\n",
    "generator = Model([z_input, label_input], note_out)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVERSARIAL MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_ambiguity_loss(y_true, y_pred):\n",
    "    even_dist = K.ones_like(y_pred) * 1.0/float(len(GENRES))\n",
    "    return K.categorical_crossentropy(even_dist, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# define input for the combined GAN model\n",
    "z_input = Input(shape=(len_input,))  # tensorflow order\n",
    "label_input = Input(shape=(1,), dtype='int32')\n",
    "img_gan = generator([z_input,label_input])\n",
    "\n",
    "# training is disable for discriminator in adversarial model\n",
    "discriminator_c.trainable = False \n",
    "\n",
    "# define output\n",
    "prediction_gan = discriminator_c(img_gan)\n",
    "print(prediction_gan)\n",
    "\n",
    "# define combined GAN model\n",
    "gan = Model([z_input,label_input], prediction_gan)\n",
    "optimizer = Adam()\n",
    "gan.compile(optimizer=optimizer, loss=['binary_crossentropy', style_ambiguity_loss], \n",
    "            loss_weights=[1.0, 0.5], metrics=['binary_accuracy'])\n",
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import pretty_midi\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Create Z for generator\n",
    "def get_noise(batch_size, len_input):\n",
    "#    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, len_input])\n",
    "    \n",
    "    # better to use a spherical Z. according to https://github.com/soumith/ganhacks\n",
    "    noise = np.random.normal(0.0, 0.50, size=[batch_size, len_input])\n",
    "    return noise\n",
    "\n",
    "def plot_drum_matrix(a):\n",
    "    if a is not None:\n",
    "        a = np.transpose(np.squeeze(a))\n",
    "        plt.matshow(a)\n",
    "        plt.show()  \n",
    "        \n",
    "        import pretty_midi\n",
    "\n",
    "def play_drum_matrix(mat, tempo=120.0):\n",
    "    # generate audio\n",
    "    audio_data = get_audio_from_drum_matrix(mat, tempo=tempo)\n",
    "    display(Audio(audio_data, rate=44100))\n",
    "    return audio_data\n",
    "\n",
    "def get_audio_from_drum_matrix(mat, tempo=120.):\n",
    "    pm = pretty_midi.PrettyMIDI(initial_tempo=tempo) # midi object\n",
    "    pm_inst = pretty_midi.Instrument(0, is_drum=True) # midi instrument\n",
    "    \n",
    "    timestep = (60./tempo) / 4. # duration of a 16th note\n",
    "    for position, timeslot in enumerate(mat):\n",
    "        for inst, onset in enumerate(timeslot):\n",
    "            if onset > 0.:\n",
    "                note_number = DRUM_MIDI_MAP[inst]\n",
    "                velocity = int(onset * 127.)\n",
    "                start = timestep * position\n",
    "                end = timestep * (position + 0.5)\n",
    "                \n",
    "                # create a midi note\n",
    "                note = pretty_midi.Note(velocity=velocity, pitch=note_number, start=start, end=end)\n",
    "                pm_inst.notes.append(note)\n",
    "    pm.instruments.append(pm_inst)\n",
    "\n",
    "    # midi -> audio\n",
    "    audio_data = pm.fluidsynth()\n",
    "    return audio_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard_logger import configure, log_value\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = \"/tmp/tf_logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "cmd = \"tensorboard --logdir=\" + logdir\n",
    "print cmd\n",
    "\n",
    "configure(logdir, flush_secs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical \n",
    "\n",
    "batch_size = 32\n",
    "nb_epochs = 500\n",
    "nb_samples = matrices_drums.shape[0]\n",
    "\n",
    "K_unrolled = 3\n",
    "MAX_LOSS_RATIO = 3.0\n",
    "\n",
    "# Labels for real drums: all ones\n",
    "real_labels = np.ones((batch_size, 1)) # * 0.9 #you can tryone-sided soft labeling for better stability\n",
    "\n",
    "# Labels for fake drums: all zeros\n",
    "fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "train_d = True\n",
    "train_g = True\n",
    "\n",
    "for epoch in range(0, nb_epochs):\n",
    "    nb_steps = int(nb_samples/batch_size)\n",
    "    \n",
    "    for repeat in range(nb_steps):\n",
    "        step = nb_steps * epoch + repeat\n",
    "        \n",
    "        if train_d:\n",
    "            \n",
    "            m_d_loss = 0.0\n",
    "            m_d_accuracy = 0.0\n",
    "            m_d_accuracy_fake, m_d_accuracy_real = 0.0, 0.0\n",
    "            m_d_loss_cat = 0.0\n",
    "            m_d_cat_accuracy = 0.0\n",
    "            \n",
    "            for j in range(K_unrolled):\n",
    "                # training data\n",
    "                random_indices = np.random.randint(0, matrices_drums.shape[0], size=batch_size)\n",
    "                drum_train = matrices_drums[random_indices, :, :]\n",
    "                labels_traing = matrices_genres[random_indices]\n",
    "                labels_traing_categorical = to_categorical(labels_traing, num_classes=NB_GENRES)\n",
    "\n",
    "                # generated samples\n",
    "                noise = get_noise(batch_size, len_input)\n",
    "                drum_fake = generator.predict([noise, labels_traing])\n",
    "\n",
    "                # training D\n",
    "                _, d_loss_real, d_loss_cat, d_acc_real, cat_accuracy = discriminator_c.train_on_batch(drum_train, [real_labels, labels_traing_categorical]) \n",
    "                _, d_loss_fake, _, d_acc_fake, _ = discriminator_c.train_on_batch(drum_fake, [fake_labels, labels_traing_categorical]) \n",
    "    \n",
    "                m_d_loss += 0.5 * (d_loss_real + d_loss_fake)\n",
    "                m_d_loss_cat += d_loss_cat\n",
    "                m_d_accuracy +=  0.5 * (d_acc_real + d_acc_fake)\n",
    "                m_d_accuracy_fake += d_acc_fake\n",
    "                m_d_accuracy_real += d_acc_real\n",
    "                m_d_cat_accuracy += cat_accuracy\n",
    "                # cache for later update\n",
    "    #             cache_weights = discriminator.get_weights()\n",
    "            \n",
    "            m_d_loss /= float(K_unrolled)\n",
    "            m_d_accuracy /= float(K_unrolled)\n",
    "            m_d_accuracy_fake /= float(K_unrolled)\n",
    "            m_d_accuracy_real /= float(K_unrolled)\n",
    "            m_d_loss_cat /= float(K_unrolled)\n",
    "            m_d_cat_accuracy /= float(K_unrolled)\n",
    "           \n",
    "            # store value\n",
    "            log_value(\"D loss\", m_d_loss, step)  \n",
    "            log_value(\"D accuracy\", m_d_accuracy, step)  \n",
    "            log_value(\"D accuracy - fake\", m_d_accuracy_fake, step) \n",
    "            log_value(\"D accuracy - real\", m_d_accuracy_real, step) \n",
    "            \n",
    "            log_value(\"D category loss\", m_d_loss_cat, step)  \n",
    "            log_value(\"D category accuracy\", m_d_cat_accuracy, step) \n",
    "            log_value(\"D total loss\", m_d_loss + m_d_loss_cat, step)\n",
    "            \n",
    "        # training G\n",
    "        if train_g:\n",
    "            y = np.ones([batch_size, 1]) # watch out the label! it should be one here        \n",
    "\n",
    "            noise = get_noise(batch_size, len_input)\n",
    "\n",
    "            # Get a batch of random labels\n",
    "            labels_random = np.random.randint(0, NB_GENRES, batch_size).reshape(-1, 1)\n",
    "\n",
    "            _, m_a_loss, m_a_cat_loss, m_a_accuracy, _ = gan.train_on_batch([noise, labels_random], [y, labels_random])\n",
    "            \n",
    "            # store value\n",
    "            log_value(\"G loss\", m_a_loss, step)\n",
    "            log_value(\"G accuracy\", m_a_accuracy, step)\n",
    "            log_value(\"style amguity loss\", m_a_cat_loss, step)\n",
    "            log_value(\"G total loss\", m_a_loss + m_a_cat_loss, step)\n",
    "        \n",
    "        if train_d and train_g:\n",
    "            if m_a_loss / m_d_loss > MAX_LOSS_RATIO:\n",
    "                train_d = False\n",
    "                print \"Pausing D\"\n",
    "            elif m_d_loss / m_a_loss > MAX_LOSS_RATIO:\n",
    "                train_g = False\n",
    "                print \"Pausing G\"\n",
    "        else:\n",
    "            train_d = True\n",
    "            train_g = True\n",
    "            \n",
    "        # update layer \n",
    "#         discriminator.set_weights(cache_weights)\n",
    " \n",
    "        if repeat % 100 == 0:\n",
    "            \n",
    "            print(\"epoch\", epoch, repeat)\n",
    "            print(\"d_loss\", m_d_loss, \"d_cat_acc\", m_d_cat_accuracy,\n",
    "                \"a_loss\", m_a_loss, 'a_cat_loss', m_a_cat_loss) # print mean loss)\n",
    "            print(\"d_accuracy\", m_d_accuracy )\n",
    "            \n",
    "            # sample output\n",
    "            noise = get_noise(1, len_input)\n",
    "            labels_random = np.random.randint(0, NB_GENRES, 1).reshape(-1, 1)\n",
    "            drum_generated = generator.predict([noise, labels_random])           \n",
    "            plot_drum_matrix(drum_generated)\n",
    "            \n",
    "            # sample audio output\n",
    "            audio_data = play_drum_matrix(np.squeeze(drum_generated))\n",
    "            wavfile.write(\"drum_%05d_%05d.wav\" % (epoch, repeat), 44100, audio_data)\n",
    "            \n",
    "            print\n",
    "            print\n",
    "            \n",
    "                        \n",
    "    # store temporary models\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        generator.save(\"./tmp/can2_pretrain_generator-epoch-%03d-%0.5f.h5\" % (epoch, m_a_loss))\n",
    "        gan.save(\"./tmp/can2_pretrain_gan-epoch-%03d-%0.5f.h5\" % (epoch, m_a_loss))\n",
    "        discriminator_c.save(\"./tmp/can2_pretrain_discriminator_c-epoch-%03d-%0.5f.h5\" % (epoch, m_d_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"./tmp/can_generator.h5\")\n",
    "gan.save(\"./tmp/can_gan.h5\")\n",
    "discriminator.save(\"./tmp/can_discriminator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    noise = get_noise(1, len_input)\n",
    "    drum_generated = generator.predict(noise)\n",
    "    plot_drum_matrix(drum_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MIDI Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi playback\n",
    "def note_matrix_to_sequence(mat, threshold = 0.5):\n",
    "    seq = []\n",
    "    for row in mat[:]:\n",
    "        arow = [[i, r] for i, r in enumerate(row) if r > threshold]\n",
    "        seq.append(arow)\n",
    "    return seq\n",
    "\n",
    "import OSC\n",
    "max_poly = 6 # maximum number of instruments played at the same time\n",
    "\n",
    "client = OSC.OSCClient()\n",
    "client.connect( ('10.0.1.14', 2014) ) \n",
    "\n",
    "def send_sequence_via_osc(seq):\n",
    "    ## the most basic ##\n",
    "    msg = OSC.OSCMessage()\n",
    "    msg.setAddress(\"/seq\")\n",
    "    msg.append(max_poly * 2)\n",
    "    \n",
    "    for notes in seq:\n",
    "        for i in range(max_poly):\n",
    "            if len(notes) > i:\n",
    "                msg.append([notes[i][0]+min_drum_note, notes[i][1]])\n",
    "            else:\n",
    "                msg.append([0, 0])\n",
    "    client.send(msg)\n",
    "    \n",
    "def playback_seq_via_osc(mat):\n",
    "    send_sequence_via_osc(note_matrix_to_sequence(mat, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "repeat = 1\n",
    "\n",
    "for j in range(100):\n",
    "    noise1 = get_noise(1, len_input) \n",
    "    noise2 = get_noise(1, len_input) \n",
    "    \n",
    "    for i in range(repeat):\n",
    "        noise = noise1 * (1.0 - i/float(repeat)) + noise2 * i/float(repeat)\n",
    "        drum_generated = generator.predict(noise)\n",
    "        mat = np.squeeze(drum_generated)\n",
    "        playback_seq_via_osc(mat)\n",
    "        time.sleep(4.0)\n",
    "        clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
